{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Predictive Maintenance\n",
    "\n",
    "This notebook handles data preprocessing for predictive maintenance tasks, including:\n",
    "- Loading CSV/zip files\n",
    "- Data resampling\n",
    "- Outlier handling\n",
    "- Feature engineering\n",
    "- Windowing for time series\n",
    "- Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import os\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_file(file_path):\n",
    "    \"\"\"Load a single CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded {file_path}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_zip_file(zip_path, extract_to='temp_extracted'):\n",
    "    \"\"\"Extract and load CSV files from a zip archive\"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        \n",
    "        csv_files = [f for f in os.listdir(extract_to) if f.endswith('.csv')]\n",
    "        dataframes = {}\n",
    "        \n",
    "        for csv_file in csv_files:\n",
    "            file_path = os.path.join(extract_to, csv_file)\n",
    "            df = load_csv_file(file_path)\n",
    "            if df is not None:\n",
    "                dataframes[csv_file] = df\n",
    "        \n",
    "        return dataframes\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing zip file: {e}\")\n",
    "        return {}\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Universal data loader for CSV or ZIP files\"\"\"\n",
    "    if file_path.endswith('.csv'):\n",
    "        return {'data': load_csv_file(file_path)}\n",
    "    elif file_path.endswith('.zip'):\n",
    "        return load_zip_file(file_path)\n",
    "    else:\n",
    "        print(\"Unsupported file format. Please use CSV or ZIP files.\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_data(df, time_col='time', freq='1H', method='mean'):\n",
    "    \"\"\"Resample time series data\"\"\"\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Time column '{time_col}' not found. Available columns: {list(df.columns)}\")\n",
    "        return df\n",
    "    \n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    df = df.set_index(time_col)\n",
    "    \n",
    "    if method == 'mean':\n",
    "        resampled = df.resample(freq).mean()\n",
    "    elif method == 'sum':\n",
    "        resampled = df.resample(freq).sum()\n",
    "    elif method == 'max':\n",
    "        resampled = df.resample(freq).max()\n",
    "    elif method == 'min':\n",
    "        resampled = df.resample(freq).min()\n",
    "    else:\n",
    "        resampled = df.resample(freq).mean()\n",
    "    \n",
    "    print(f\"Resampled data from {len(df)} to {len(resampled)} rows\")\n",
    "    return resampled.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(df, threshold=3, columns=None):\n",
    "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    outlier_mask = np.zeros(len(df), dtype=bool)\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            z_scores = np.abs(stats.zscore(df[col].dropna()))\n",
    "            outlier_mask |= (z_scores > threshold)\n",
    "    \n",
    "    return outlier_mask\n",
    "\n",
    "def detect_outliers_iqr(df, columns=None):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    outlier_mask = np.zeros(len(df), dtype=bool)\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outlier_mask |= ((df[col] < lower_bound) | (df[col] > upper_bound))\n",
    "    \n",
    "    return outlier_mask\n",
    "\n",
    "def handle_outliers(df, method='clip', outlier_method='iqr', columns=None):\n",
    "    \"\"\"Handle outliers by removing, clipping, or imputing\"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if outlier_method == 'zscore':\n",
    "        outlier_mask = detect_outliers_zscore(df, columns=columns)\n",
    "    else:\n",
    "        outlier_mask = detect_outliers_iqr(df, columns=columns)\n",
    "    \n",
    "    if method == 'remove':\n",
    "        df_clean = df[~outlier_mask].copy()\n",
    "        print(f\"Removed {outlier_mask.sum()} outliers\")\n",
    "    elif method == 'clip':\n",
    "        df_clean = df.copy()\n",
    "        for col in columns:\n",
    "            if col in df.columns:\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                df_clean[col] = np.clip(df[col], lower_bound, upper_bound)\n",
    "        print(f\"Clipped outliers in {len(columns)} columns\")\n",
    "    elif method == 'median':\n",
    "        df_clean = df.copy()\n",
    "        for col in columns:\n",
    "            if col in df.columns:\n",
    "                median_val = df[col].median()\n",
    "                df_clean.loc[outlier_mask, col] = median_val\n",
    "        print(f\"Imputed outliers with median in {len(columns)} columns\")\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_features(df, columns=None, windows=[5, 10, 20]):\n",
    "    \"\"\"Create rolling statistics features\"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            for window in windows:\n",
    "                df_featured[f'{col}_rolling_mean_{window}'] = df[col].rolling(window=window).mean()\n",
    "                df_featured[f'{col}_rolling_std_{window}'] = df[col].rolling(window=window).std()\n",
    "                df_featured[f'{col}_rolling_min_{window}'] = df[col].rolling(window=window).min()\n",
    "                df_featured[f'{col}_rolling_max_{window}'] = df[col].rolling(window=window).max()\n",
    "    \n",
    "    print(f\"Created rolling features for {len(columns)} columns with windows {windows}\")\n",
    "    return df_featured\n",
    "\n",
    "def create_lag_features(df, columns=None, lags=[1, 2, 3]):\n",
    "    \"\"\"Create lag features\"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            for lag in lags:\n",
    "                df_featured[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "    \n",
    "    print(f\"Created lag features for {len(columns)} columns with lags {lags}\")\n",
    "    return df_featured\n",
    "\n",
    "def create_difference_features(df, columns=None):\n",
    "    \"\"\"Create difference features\"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df_featured[f'{col}_diff'] = df[col].diff()\n",
    "            df_featured[f'{col}_pct_change'] = df[col].pct_change()\n",
    "    \n",
    "    print(f\"Created difference features for {len(columns)} columns\")\n",
    "    return df_featured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Windowing for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(df, window_size=30, step_size=1, target_col=None):\n",
    "    \"\"\"Create sliding windows for time series prediction\"\"\"\n",
    "    windows = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(0, len(df) - window_size, step_size):\n",
    "        window = df.iloc[i:i+window_size].values\n",
    "        windows.append(window)\n",
    "        \n",
    "        if target_col and i + window_size < len(df):\n",
    "            target = df.iloc[i + window_size][target_col]\n",
    "            targets.append(target)\n",
    "    \n",
    "    windows = np.array(windows)\n",
    "    \n",
    "    if target_col:\n",
    "        targets = np.array(targets)\n",
    "        print(f\"Created {len(windows)} windows with targets\")\n",
    "        return windows, targets\n",
    "    else:\n",
    "        print(f\"Created {len(windows)} windows\")\n",
    "        return windows\n",
    "\n",
    "def create_sequences_for_lstm(df, sequence_length=50, target_col=None):\n",
    "    \"\"\"Create sequences specifically for LSTM models\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(df) - sequence_length):\n",
    "        seq = df.iloc[i:i+sequence_length].values\n",
    "        sequences.append(seq)\n",
    "        \n",
    "        if target_col:\n",
    "            target = df.iloc[i+sequence_length][target_col]\n",
    "            targets.append(target)\n",
    "    \n",
    "    sequences = np.array(sequences)\n",
    "    \n",
    "    if target_col:\n",
    "        targets = np.array(targets)\n",
    "        print(f\"Created {len(sequences)} LSTM sequences with targets\")\n",
    "        return sequences, targets\n",
    "    else:\n",
    "        print(f\"Created {len(sequences)} LSTM sequences\")\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df, method='standard', columns=None):\n",
    "    \"\"\"Normalize numerical columns\"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    df_normalized = df.copy()\n",
    "    \n",
    "    if method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        print(\"Unknown normalization method. Using standard scaling.\")\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    df_normalized[columns] = scaler.fit_transform(df[columns])\n",
    "    \n",
    "    print(f\"Normalized {len(columns)} columns using {method} scaling\")\n",
    "    return df_normalized, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_pipeline(file_path, config=None):\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    if config is None:\n",
    "        config = {\n",
    "            'resample': {'freq': '1H', 'method': 'mean'},\n",
    "            'outliers': {'method': 'clip', 'outlier_method': 'iqr'},\n",
    "            'features': {'rolling_windows': [5, 10], 'lags': [1, 2]},\n",
    "            'normalize': {'method': 'standard'}\n",
    "        }\n",
    "    \n",
    "    # Load data\n",
    "    data_dict = load_data(file_path)\n",
    "    \n",
    "    processed_data = {}\n",
    "    \n",
    "    for name, df in data_dict.items():\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing {name}...\")\n",
    "        \n",
    "        # Resample\n",
    "        if 'resample' in config:\n",
    "            df = resample_data(df, **config['resample'])\n",
    "        \n",
    "        # Handle outliers\n",
    "        if 'outliers' in config:\n",
    "            df = handle_outliers(df, **config['outliers'])\n",
    "        \n",
    "        # Feature engineering\n",
    "        if 'features' in config:\n",
    "            if 'rolling_windows' in config['features']:\n",
    "                df = create_rolling_features(df, windows=config['features']['rolling_windows'])\n",
    "            if 'lags' in config['features']:\n",
    "                df = create_lag_features(df, lags=config['features']['lags'])\n",
    "            df = create_difference_features(df)\n",
    "        \n",
    "        # Normalize\n",
    "        if 'normalize' in config:\n",
    "            df, scaler = normalize_data(df, **config['normalize'])\n",
    "        \n",
    "        # Drop NaN values created by feature engineering\n",
    "        df = df.dropna()\n",
    "        \n",
    "        processed_data[name] = df\n",
    "        print(f\"Final shape: {df.shape}\")\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment and modify for your data)\n",
    "# file_path = 'path/to/your/data.csv'  # or .zip\n",
    "# processed_data = preprocess_data_pipeline(file_path)\n",
    "# \n",
    "# # For windowing (if needed for model training)\n",
    "# if 'data' in processed_data:\n",
    "#     df = processed_data['data']\n",
    "#     windows, targets = create_sliding_windows(df, window_size=30, target_col='target_column')\n",
    "#     print(f\"Windows shape: {windows.shape}, Targets shape: {targets.shape}\")\n",
    "\n",
    "print(\"Data preprocessing functions defined. Ready to use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
